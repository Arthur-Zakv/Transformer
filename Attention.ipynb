{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d89ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d0462",
   "metadata": {},
   "source": [
    "# LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ae5c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_embedder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.encoder = nn.LSTM(hidden_dim, hidden_dim)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        enc_input = self.input_embedder(x)\n",
    "        out, (h, c) = self.encoder(enc_input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "b7e137cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        \n",
    "        self.MLP_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def attention(self, h_enc, h_dec):\n",
    "        q = self.MLP_q(h_dec).unsqueeze(1)\n",
    "    \n",
    "        score_list = list()\n",
    "        for t in range(ht):\n",
    "            k = self.MLP_k(h_enc[:,t,:]).unsqueeze(1)\n",
    "            \n",
    "            q_dot_k = torch.bmm(q, k.permute(0,2,1)).squeeze(2).squeeze(1)\n",
    "            sqrt_H = torch.tensor(hidden_dim).float().sqrt()\n",
    "            score = q_dot_k / sqrt_H \n",
    "            score_list.append(score)\n",
    "            \n",
    "\n",
    "        score = torch.stack(score_list, dim = 1)\n",
    "        att_q_k = nn.Softmax(dim = 1)(score).unsqueeze(2)\n",
    "        return att_q_k\n",
    "    \n",
    "    def context(self, att, h_enc):\n",
    "        v = self.MLP_v(h_enc)\n",
    "        context = (att * v).sum(dim = 1)\n",
    "        return context\n",
    "    \n",
    "    def forward(self, h_enc, h_dec):\n",
    "        attention = self.attention(h_enc, h_dec)\n",
    "        context = self.context(attention, h_enc)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "id": "9febe21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        \n",
    "        self.Wq_dot_ = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wk_dot_ = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wa_dot_ = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, h_enc, h_dec):\n",
    "        \n",
    "        # Calculate Attention\n",
    "        q = h_dec\n",
    "        score_list = list()\n",
    "        for t in range(ht):\n",
    "            k = h_enc[:,t,:]\n",
    "            Wk_dot_k = self.Wk_dot_(k)\n",
    "            \n",
    "            q = h_dec.squeeze()\n",
    "            Wq_dot_q = self.Wq_dot_(q)\n",
    "            \n",
    "            a = nn.Tanh()(Wq_dot_q + Wk_dot_k)\n",
    "            wa_dot_a = self.wa_dot_(a)\n",
    "            \n",
    "            score_list.append(wa_dot_a)\n",
    "        score = torch.stack(score_list, dim = 1)\n",
    "            \n",
    "        att_q_k = nn.Softmax(dim = 1)(score)\n",
    "        \n",
    "        # Calculate Context Vector\n",
    "        v = h_enc\n",
    "        context = (att_q_k * v).sum(dim = 1)\n",
    "            \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "c7913c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder_cell = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "        #self.attention = AdditiveAttention()\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.output_embedder = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, h_enc):\n",
    "        \n",
    "        h = torch.zeros(h_enc.shape[0], hidden_dim)\n",
    "        c = torch.zeros(h_enc.shape[0], hidden_dim)\n",
    "        \n",
    "        h_dec = list()\n",
    "        for t in range(ft):\n",
    "            x = self.attention(h_enc, h)\n",
    "            h, c = self.decoder_cell(x, (h,c))\n",
    "            h_dec.append(h)\n",
    "            \n",
    "        h_dec = torch.stack(h_dec, dim = 1)\n",
    "        \n",
    "        logits = self.output_embedder(h_dec)\n",
    "        log_pis = nn.LogSoftmax(dim = -1)(logits)\n",
    "        return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1af28cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h_enc = self.encoder(x)\n",
    "        y = self.decoder(h_enc)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "3c0a4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(log_p_y_x, y):\n",
    "    \n",
    "    y_OH = batch_to_one_hot(y, num_cats)\n",
    "    E_i_t = (log_p_y_x * y_OH).sum(dim = 2) \n",
    "    E_i = E_i_t.sum(dim = 1) \n",
    "    E = E_i.mean(dim = 0)\n",
    "    \n",
    "    return -E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "785e1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256\n",
    "\n",
    "input_dim = 10\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "\n",
    "ht = 8\n",
    "ft = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89b3af",
   "metadata": {},
   "source": [
    "# Attention is All you need paper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "54ce40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_one_hot(batch_cat_id, num_cats):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    batch_cat_id : torch.tensor [bs, seq_len, 1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    batch_cat_OH : torch.tensor [bs, seq_len, num_cats]\n",
    "    \n",
    "    \"\"\"\n",
    "    cat_samples = batch_cat_id.chunk(len(batch_cat_id), dim = 0)\n",
    "    batch_cat_OH = list()\n",
    "    for cat_sample in cat_samples:\n",
    "        cat_id = cat_sample.squeeze()\n",
    "        cat_OH = torch.zeros(len(cat_id), num_cats)\n",
    "        cat_OH[torch.arange(len(cat_id)), cat_id] = 1\n",
    "        batch_cat_OH.append(cat_OH)\n",
    "\n",
    "    return torch.stack(batch_cat_OH, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "999bf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"Single Attention Head\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    Q : torch.tensor [bs, seq_len, d_model]\n",
    "    K : torch.tensor [bs, seq_len, d_model]\n",
    "    V : torch.tensor [bs, seq_len, d_model]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y : torch.tensor [bs, seq_len, d_v]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        \n",
    "        self.query_weights = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_q) \n",
    "            for ts in range(max_seq_len)\n",
    "        ])\n",
    "        \n",
    "        self.key_weights = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_k) \n",
    "            for ts in range(max_seq_len)\n",
    "        ])\n",
    "        \n",
    "        self.value_weights = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_v) \n",
    "            for ts in range(max_seq_len)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def linear(self, Q, K, V):\n",
    "\n",
    "        Q = torch.stack([self.query_weights[ts](Q[:,ts,:])\n",
    "                        for ts in range(Q.shape[1])], \n",
    "                        dim = 1)\n",
    "        \n",
    "        K = torch.stack([self.key_weights[ts](K[:,ts,:])\n",
    "                        for ts in range(K.shape[1])], \n",
    "                        dim = 1)\n",
    "        \n",
    "        V = torch.stack([self.value_weights[ts](V[:,ts,:])\n",
    "                        for ts in range(V.shape[1])], \n",
    "                        dim = 1)\n",
    "        return Q, K, V\n",
    "    \n",
    "    \n",
    "    def attention(self, Q, K, V):\n",
    "        S = torch.bmm(Q, K.permute(0,2,1)) / torch.sqrt(torch.tensor(d_k))\n",
    "        W = nn.Softmax(dim = 2)(S)\n",
    "        Y = torch.bmm(W, V)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        Q, K, V = self.linear(Q, K, V)\n",
    "        Y = self.attention(Q, K, V)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7e1ae665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSingleHeadAttention(SingleHeadAttention):\n",
    "    \"\"\"Single Attention Head\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    Q : torch.tensor [bs, seq_len, d_model]\n",
    "    K : torch.tensor [bs, seq_len, d_model]\n",
    "    V : torch.tensor [bs, seq_len, d_model]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y : torch.tensor [bs, seq_len, d_v]\n",
    "    \n",
    "    \n",
    "    Difference Compared to Regular SingleHeadAttention is the changed Attention weight Matrix.\n",
    "    It ensures that every Prediction timestep has only access to itself and previous timesteps.\n",
    "    \n",
    "    [w 0 0 0 0]    |\n",
    "    [w w w 0 0]   output\n",
    "    [w w w w 0]    |\n",
    "    [w w w w w]    |\n",
    "    \n",
    "    <--input-->\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MaskedSingleHeadAttention, self).__init__()\n",
    "    \n",
    "    def put_on_mask(self, S):\n",
    "        \n",
    "        actual_seq_len = S.shape[1]\n",
    "        \n",
    "        mask = torch.ones((actual_seq_len, actual_seq_len))\n",
    "        neg_infs = torch.zeros((actual_seq_len, actual_seq_len))\n",
    "        \n",
    "        for t in range(actual_seq_len):\n",
    "            for d in range(actual_seq_len):\n",
    "                if d > t:\n",
    "                    mask[t,d] = 0\n",
    "                    neg_infs[t, d] = -np.inf\n",
    "                    \n",
    "        S_masked = S * mask + neg_infs\n",
    "        return S_masked\n",
    "    \n",
    "    \n",
    "    \n",
    "    def attention(self, Q, K, V):\n",
    "        S = torch.bmm(Q, K.permute(0,2,1)) / torch.sqrt(torch.tensor(d_k))\n",
    "        S = self.put_on_mask(S)\n",
    "        W = nn.Softmax(dim = 2)(S)\n",
    "        Y = torch.bmm(W, V)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5f62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.h = h\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            SingleHeadAttention()\n",
    "            for _ in range(h)\n",
    "        ])\n",
    "        \n",
    "        self.dense = nn.Linear(h * d_v, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        y_SH_cat = torch.cat([self.heads[i](Q, K, V) \n",
    "                           for i in range(self.h)], \n",
    "                          dim = 2)\n",
    "        y_MH = self.dense(y_SH_cat)\n",
    "        y_MH = nn.Softmax(dim = 2)(y_MH)\n",
    "        return y_MH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1331ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(MultiHeadAttention):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "                \n",
    "        self.heads = nn.ModuleList([\n",
    "            MaskedSingleHeadAttention()\n",
    "            for _ in range(h)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd86196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.linear_in = nn.Linear(d_model, d_ff)\n",
    "        self.linear_out = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = nn.ReLU()(self.linear_in(x))\n",
    "        y = self.linear_out(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1abe1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single Encoder Layer \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    multi_head_attention\n",
    "    add_and_norm\n",
    "    \n",
    "    feed_forward \n",
    "    add_and_norm\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention()\n",
    "        self.add_and_norm_MHA = AddAndNorm()\n",
    "        \n",
    "        self.feed_forward = FeedForward()\n",
    "        self.add_and_norm_ff = AddAndNorm()\n",
    "        \n",
    "    def forward(self, x_emb):\n",
    "        MHA = self.multi_head_attention(Q = x_emb, K = x_emb, V = x_emb)\n",
    "        MHA_norm = self.add_and_norm_MHA(MHA, x_emb)\n",
    "        \n",
    "        ff = self.feed_forward(MHA_norm)\n",
    "        z = self.add_and_norm_ff(ff, MHA_norm)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "23f39e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single Decoder Layer \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    masked_multi_head_attention \n",
    "    add_and_norm\n",
    "    \n",
    "    multi_head_attention\n",
    "    add_and_norm\n",
    "    \n",
    "    feed_forward \n",
    "    add_and_norm\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.masked_multi_head_attention = MaskedMultiHeadAttention()\n",
    "        self.add_and_norm_MMHA = AddAndNorm()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention()\n",
    "        self.add_and_norm_MHA = AddAndNorm()\n",
    "        \n",
    "        self.feed_forward = FeedForward()\n",
    "        self.add_and_norm_ff = AddAndNorm()\n",
    "    \n",
    "    def forward(self, output, latent):\n",
    "        MMHA = self.masked_multi_head_attention(Q = output, K = output, V = output)\n",
    "        MMHA_norm = self.add_and_norm_MMHA(output, MMHA)\n",
    "\n",
    "        MHA = self.multi_head_attention(Q = MMHA_norm, K = latent, V = latent)\n",
    "        MHA_norm = self.add_and_norm_MHA(MMHA_norm, MHA)\n",
    "        \n",
    "        ff = self.feed_forward(MHA_norm)\n",
    "        ff_norm = self.add_and_norm_ff(ff, MHA_norm)\n",
    "        \n",
    "        return ff_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "6a295588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    \"\"\"Add State and it's copy that has passed through Attention and use Layer Normalization\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        self.normalize = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        return self.normalize(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0a71c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Sequence of Encoder Layers\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    N : int\n",
    "        Number Encoder Layers\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer()\n",
    "            for _ in range(N)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x_emb):\n",
    "        z = x_emb\n",
    "        N_z = list()\n",
    "        for layer in self.layers:\n",
    "            z = layer(z)\n",
    "            N_z.append(z)\n",
    "            \n",
    "        return N_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9ed75f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\"Sequence of Decoder Layers\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    N : int\n",
    "        Number Decoder Layers\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer()\n",
    "            for _ in range(N)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, y, N_z):    \n",
    "        for n, layer in enumerate(self.layers):\n",
    "            z = N_z[n]\n",
    "            y = layer(y,z) \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "48198959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer - Timeseries Forecasting\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    task_type : str \n",
    "    \n",
    "        'regression' : input / output OHE [bs, time, cat]   \n",
    "        'classification' : input / output [bs, time, dim]\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    run_encoder()\n",
    "        Run all parts of the Encoding process\n",
    "        \n",
    "    run_decoder_inference()\n",
    "        Run all parts of the Decoding process in the Prediction Mode\n",
    "        \n",
    "    run_decoder_train()\n",
    "        Run all parts of the Decoding process in the Training Mode, when labels are known in advance\n",
    "        \n",
    "    predict()\n",
    "        Run full Trainsformer in the Prediction Mode, returning a prediction\n",
    "        \n",
    "    train_loss()\n",
    "        Run full Trainsformer in the Training Mode, returning the training loss\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.task_type = 'regression'# 'classification' \n",
    "        \n",
    "        self.enc_input_embedder = nn.Linear(d_input, d_model)\n",
    "        self.encoder = Encoder()\n",
    "        \n",
    "        self.dec_ouput_embedder = nn.Linear(d_input, d_model)\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        self.dec_linear = nn.Linear(d_model, d_output)\n",
    "                \n",
    "    def run_encoder(self, x):\n",
    "        \"\"\"Run All parts of the Encoding process\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.tensor [bs, ht, d_input]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        N_z : list of torch.tensor [bs, ht, d_model]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        x_emb = self.enc_input_embedder(x)\n",
    "        x_PE = positional_encoding(seq_len, d_model)\n",
    "        x_enc_in = x_emb + x_PE\n",
    "        \n",
    "        N_z = self.encoder(x_enc_in)\n",
    "        return N_z\n",
    "    \n",
    "    def run_decoder_inference(self, y_previous, N_z):\n",
    "        \"\"\" Decoder Inference Mode: Iteratively\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        N_z : list of torch.tensor [bs, ht, d_model]\n",
    "            Encoder outputs of history sequence of each of the N layers.\n",
    "            \n",
    "        y_previous : torch.tensor [bs, 1, d_input] \n",
    "            Initial decoder input. \n",
    "            We just copy the last timestep of the history sequence here.\n",
    "            \n",
    "        Procedure\n",
    "        ---------\n",
    "        First input to the decoder is just like start token.\n",
    "        The last output timestep of the decoder gets apppended to the input for the next iteration.\n",
    "        With every Iteration, both the in and output sequence grow, until the desired sequence length is reached.\n",
    "        The last output sequence is the final output.\n",
    "        \n",
    "        Iteration 1: in:[y0]       > out:[y1] \n",
    "        Iteration 2: in:[y0,y1]    > out:[y1,y2]\n",
    "        Iteration 3: in:[y0,y1,y2] > out:[y1,y2,y3]\n",
    "        \n",
    "        y_pred = [y1, y2, ..., y_ft]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : torch.tensor [bs, ft, d_output]\n",
    "        \n",
    "        \"\"\"\n",
    "        for t in range(ft):\n",
    "            \n",
    "            # embedding\n",
    "            y_emb = self.dec_ouput_embedder(y_previous)\n",
    "            y_PE = positional_encoding(y_emb.shape[1], d_model)\n",
    "            y_dec_in = y_emb + y_PE\n",
    "            \n",
    "            # decoder \n",
    "            y_next = self.decoder(y_dec_in, N_z)\n",
    "            y_next = y_next[:,-1,:].unsqueeze(1)\n",
    "            y_next = self.dec_linear(y_next)   \n",
    "            \n",
    "            # Ouput Distribution\n",
    "            if self.task_type == 'classification':\n",
    "                p_y_x = nn.Softmax(dim = -1)(y_next)\n",
    "                y_next = p_y_x\n",
    "                \n",
    "            if self.task_type == 'regression':\n",
    "                # hier gegebenfalls Normal distribution aufstellen\n",
    "                mu = y_next\n",
    "                y_next = mu\n",
    "                \n",
    "            y_previous = torch.cat([y_previous, y_next], dim = 1)\n",
    "                \n",
    "        y_pred = y_previous[:,1:,:]\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            cat_id = y_pred.argmax(dim = -1).unsqueeze(-1)\n",
    "            cat_OH = batch_to_one_hot(cat_id, d)\n",
    "            return cat_OH\n",
    "                \n",
    "        if self.task_type == 'regression':\n",
    "            return y_pred\n",
    "        \n",
    "    \n",
    "    def run_decoder_train(self, y_0, y_gt, N_z):\n",
    "        \n",
    "        \"\"\" Decoder Training Mode: Simultaniously all timesteps\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        N_z : list of torch.tensor [bs, ht, d_model]\n",
    "            Encoder outputs of history sequence of each of the N layers.\n",
    "            \n",
    "        y_0 : torch.tensor [bs, 1, d_input] \n",
    "            Initial decoder input. \n",
    "            We just copy the last timestep of the history sequence here.\n",
    "            \n",
    "        y_gt : torch.tensor [bs, ft, d_output]\n",
    "            \n",
    "        Procedure\n",
    "        ---------\n",
    "        All timesteps are calculated simultaneously.\n",
    "        \n",
    "        \n",
    "        in: [y0, y1, ..., y_ft-1] -> out: [y1, y2, ... , y_ft]\n",
    "        \n",
    "          cat[y0 | y_gt[:-1]]     ->           y_gt\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : torch.tensor [bs, ft, d_output]\n",
    "        \n",
    "        \"\"\"\n",
    "        # concat first token to ground truth\n",
    "        y_shifted_right = torch.cat([y_0, y_gt[:,:-1,:]], dim = 1)\n",
    "        \n",
    "        # embedder\n",
    "        y_emb = self.dec_ouput_embedder(y_shifted_right)\n",
    "        y_PE = positional_encoding(y_emb.shape[1], d_model)\n",
    "        y_dec_in = y_emb + y_PE\n",
    "        \n",
    "        # decoder\n",
    "        y_pred = self.decoder(y_dec_in, N_z)\n",
    "        y_pred = self.dec_linear(y_pred)\n",
    "                \n",
    "        # Ouput Distribution + Loss\n",
    "        if self.task_type == 'classification':\n",
    "            p_y_x = nn.Softmax(dim = -1)(y_pred)\n",
    "            loss = CESequenceLoss(p_y_x, y_gt)\n",
    "            \n",
    "        if self.task_type == 'regression':\n",
    "            # Hier gegebnfalls Normal Distribution\n",
    "            mu = y_pred\n",
    "            y_pred = mu\n",
    "            loss = MSESequenceLoss(y_pred, y_gt)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, x):\n",
    "        N_z = self.run_encoder(x)\n",
    "        first_dec_out = x[:,-1,:].unsqueeze(1)\n",
    "        y_pred = self.run_decoder_inference(first_dec_out, N_z)\n",
    "        return y_pred\n",
    "    \n",
    "    def train_loss(self, x, y_gt):\n",
    "        N_z = self.run_encoder(x)\n",
    "        y_0 = x[:,-1,:].unsqueeze(1)\n",
    "        loss = self.run_decoder_train(y_0, y_gt, N_z)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2a29c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CESequenceLoss(p_y_x, y):\n",
    "    log_p_y_x = torch.log(p_y_x).clamp(min = -100)\n",
    "    E_i_t = - (y * log_p_y_x).sum(dim = 2)\n",
    "    E_i = E_i_t.sum(dim = 1)\n",
    "    E = E_i.mean(dim = 0)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f80b2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSESequenceLoss(y_pred, y):\n",
    "    SE_dim_total = ((y_pred - y) ** 2).sum(2)\n",
    "    SE_seq_total = SE_dim_total.sum(1)\n",
    "    MSE = SE_seq_total.mean(dim = 0)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3f35b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    def p(t,k):\n",
    "        def is_even(x):\n",
    "            return x % 2 == 0\n",
    "        def w(k):\n",
    "            return torch.tensor(1/math.pow(10000, 2*k / d_model))\n",
    "\n",
    "        if is_even(k):\n",
    "            return torch.sin(w(k) * t)\n",
    "        if not is_even(k):\n",
    "            return torch.cos(w(k) * t)\n",
    "        \n",
    "    P = torch.zeros((seq_len, d_model))\n",
    "    for t in range(seq_len):\n",
    "        for k in range(d_model):\n",
    "            P[t,k] = p(t,k)\n",
    "    return P\n",
    "\n",
    "def binary_to_float(x):\n",
    "    d = len(x)\n",
    "    return torch.tensor([math.pow(2, idx) * x[idx] for idx in range(d)]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "73288bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "ht = 10\n",
    "ft = 10\n",
    "\n",
    "seq_len = ht\n",
    "max_seq_len = 10 # 512\n",
    "N = 1\n",
    "h = 1\n",
    "\n",
    "d_input = 1\n",
    "d_output = 1\n",
    "d_model = 512\n",
    "d_ff = 4 * d_model\n",
    "\n",
    "d_k = int(d_model / h)\n",
    "d_v = int(d_model / h)\n",
    "d_q = int(d_model / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "35dd9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Classification\n",
    "x = torch.randint(0, d-1, (1,ht,1))\n",
    "x_OH = batch_to_one_hot(x, d)\n",
    "\n",
    "y = torch.randint(0, d-1, (1,ft,1))\n",
    "y_OH = batch_to_one_hot(y, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3190c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Regression\n",
    "t = torch.arange(0, 20, 1)\n",
    "y = torch.sin(t)\n",
    "hist = torch.stack([y[:10]], dim = 1).unsqueeze(0)\n",
    "fut = torch.stack([y[10:]], dim = 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "75945fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 4.505513668060303\n",
      "loss = 5.025459289550781\n",
      "loss = 3.1072521209716797\n",
      "loss = 1.6260898113250732\n",
      "loss = 1.6819764375686646\n",
      "loss = 1.0738043785095215\n",
      "loss = 0.44290584325790405\n",
      "loss = 0.4375051259994507\n",
      "loss = 0.049974408000707626\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-b410be373abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Transformer()\n",
    "optimizer = Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = StepLR(optimizer, 100, 0.9)\n",
    "\n",
    "#Training\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    loss = model.train_loss(hist, fut)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'loss = {loss.detach()}')\n",
    "\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
